{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ba7307",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec392536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39aa878",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28d067c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class CustomEEGDataset(Dataset):\n",
    "    def __init__(self, annotations_file):\n",
    "        df = pd.read_csv(annotations_file)\n",
    "\n",
    "        self.X = df.iloc[:, 1:-1].to_numpy(dtype=np.float32)\n",
    "        y = df.iloc[:, -1]\n",
    "        y = y.astype(np.int64)\n",
    "        self.y = y.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.item()\n",
    "        x = torch.from_numpy(self.X[idx]).unsqueeze(0)\n",
    "        y = torch.as_tensor(self.y[idx])\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77e48c",
   "metadata": {},
   "source": [
    "## Model and Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a879864",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    csv_path: str = \"../data/data_preprocessed.csv\"\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 20\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 2\n",
    "    model_out: str = \"model.pt\"\n",
    "    threshold_out: str = \"threshold.json\"\n",
    "\n",
    "class TinyEEGCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: [B, 1, L]\n",
    "    Output: raw logit per sample (use BCEWithLogitsLoss)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 16, kernel_size=7, padding=3), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),    nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),    nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)  # -> [B, 64, 1]\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, 1)  # -> [B, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x).squeeze(-1)  # [B, 64]\n",
    "        logit = self.classifier(x)        # [B, 1]\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746806a",
   "metadata": {},
   "source": [
    "## Training/Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b03c4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, loss_fn, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)                  # [B, 1, L]\n",
    "        y = y.to(device).float().unsqueeze(1)  # [B, 1]\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)                  # [B, 1]\n",
    "        loss = loss_fn(logit, y)\n",
    "        loss.backward()\n",
    "        if grad_clip is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_probs(model, loader, device):\n",
    "    model.eval()\n",
    "    probs, ys = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logit = model(x)\n",
    "        p = torch.sigmoid(logit).squeeze(1).cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ys.append(y.numpy())\n",
    "    return np.concatenate(ys), np.concatenate(probs)\n",
    "\n",
    "def evaluate_probs(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:\n",
    "    y_true = y_true.astype(int)\n",
    "    # AUROC can fail if only one class present; guard it\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    # default threshold 0.5 for reporting\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, preds, average=\"binary\", zero_division=0)\n",
    "    return {\"auc\": auc, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227afe7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd5c7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainConfig()\n",
    "\n",
    "eeg_dataset = CustomEEGDataset(cfg.csv_path)\n",
    "train_dataset, test_dataset = random_split(eeg_dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "neg = (train_dataset[:][1] == 0).sum()\n",
    "pos = (train_dataset[:][1] == 1).sum()\n",
    "pos_weight = torch.tensor([neg / max(pos, 1)], dtype=torch.float32, device=cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98c39f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyEEGCNN().to(cfg.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "53bb651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved with F1: 0.9809\n",
      "[01] loss=0.0591 test_f1=0.981 test_auc=0.999 test_acc=0.992\n",
      "[02] loss=0.0619 test_f1=0.980 test_auc=0.999 test_acc=0.992\n",
      "New best model saved with F1: 0.9820\n",
      "[03] loss=0.0610 test_f1=0.982 test_auc=1.000 test_acc=0.993\n",
      "[04] loss=0.0618 test_f1=0.981 test_auc=0.999 test_acc=0.992\n",
      "[05] loss=0.0614 test_f1=0.980 test_auc=0.999 test_acc=0.992\n",
      "[06] loss=0.0603 test_f1=0.981 test_auc=1.000 test_acc=0.992\n",
      "New best model saved with F1: 0.9841\n",
      "[07] loss=0.0603 test_f1=0.984 test_auc=1.000 test_acc=0.993\n",
      "[08] loss=0.0574 test_f1=0.983 test_auc=1.000 test_acc=0.993\n",
      "[09] loss=0.0546 test_f1=0.982 test_auc=1.000 test_acc=0.993\n",
      "[10] loss=0.0568 test_f1=0.981 test_auc=0.999 test_acc=0.992\n",
      "[11] loss=0.0580 test_f1=0.984 test_auc=1.000 test_acc=0.993\n",
      "[12] loss=0.0533 test_f1=0.981 test_auc=1.000 test_acc=0.992\n",
      "[13] loss=0.0527 test_f1=0.983 test_auc=1.000 test_acc=0.993\n",
      "[14] loss=0.0560 test_f1=0.980 test_auc=0.999 test_acc=0.992\n",
      "[15] loss=0.0518 test_f1=0.981 test_auc=1.000 test_acc=0.992\n",
      "[16] loss=0.0564 test_f1=0.982 test_auc=0.999 test_acc=0.993\n",
      "[17] loss=0.0582 test_f1=0.983 test_auc=0.999 test_acc=0.993\n",
      "[18] loss=0.0564 test_f1=0.983 test_auc=0.999 test_acc=0.993\n",
      "[19] loss=0.0546 test_f1=0.982 test_auc=1.000 test_acc=0.993\n",
      "[20] loss=0.0503 test_f1=0.983 test_auc=1.000 test_acc=0.993\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, cfg.device, cfg.grad_clip)\n",
    "    y_test, p_test = infer_probs(model, test_loader, cfg.device)\n",
    "    metrics = evaluate_probs(y_test, p_test)\n",
    "\n",
    "    # Save the best model based on F1 score\n",
    "    if metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = metrics[\"f1\"]\n",
    "        # Save the model state dict\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "            'metrics': metrics\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
    "\n",
    "    scheduler.step(metrics[\"f1\"])\n",
    "\n",
    "    print(f\"[{epoch:02d}] loss={train_loss:.4f} test_f1={metrics['f1']:.3f} test_auc={metrics['auc']:.3f} \"\n",
    "          f\"test_acc={metrics['acc']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c557f39",
   "metadata": {},
   "source": [
    "# Model Training v2: Grouping By Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29b9f275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient\n",
       "X21    500\n",
       "X15    500\n",
       "X8     500\n",
       "X16    500\n",
       "X20    500\n",
       "X14    500\n",
       "X3     500\n",
       "X11    500\n",
       "X19    500\n",
       "X7     500\n",
       "X1     500\n",
       "X22    500\n",
       "X9     500\n",
       "X23    500\n",
       "X18    500\n",
       "X2     500\n",
       "X12    500\n",
       "X5     500\n",
       "X10    500\n",
       "X13    500\n",
       "X4     500\n",
       "X17    500\n",
       "X6     500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = TrainConfig()\n",
    "df = pd.read_csv(cfg.csv_path)\n",
    "df['Patient'] = df['Unnamed'].str.extract(r\"(.*)\\.V[0-9]+\\.*\")\n",
    "df['Patient'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee86e73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    X21.V1.791\n",
       "1    X15.V1.924\n",
       "2       X8.V1.1\n",
       "3     X16.V1.60\n",
       "4     X20.V1.54\n",
       "Name: Unnamed, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Unnamed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5bbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
