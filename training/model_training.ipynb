{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ba7307",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec392536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28d067c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class CustomEEGDataset(Dataset):\n",
    "    def __init__(self, annotations_file):\n",
    "        df = pd.read_csv(annotations_file)\n",
    "\n",
    "        self.X = df.iloc[:, 1:-1].to_numpy(dtype=np.float32)\n",
    "        y = df.iloc[:, -1]\n",
    "        y = y.astype(np.int64)\n",
    "        self.y = y.to_numpy()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.item()\n",
    "        x = torch.from_numpy(self.X[idx]).unsqueeze(0)\n",
    "        y = torch.as_tensor(self.y[idx])\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a879864",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    csv_path: str = \"../data/data_preprocessed.csv\"\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 20\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: float = 1.0\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_workers: int = 2\n",
    "    model_out: str = \"model.pt\"\n",
    "    threshold_out: str = \"threshold.json\"\n",
    "\n",
    "class TinyEEGCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: [B, 1, L]\n",
    "    Output: raw logit per sample (use BCEWithLogitsLoss)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch=1):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 16, kernel_size=7, padding=3), nn.BatchNorm1d(16), nn.ReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),    nn.BatchNorm1d(32), nn.ReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),    nn.BatchNorm1d(64), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)  # -> [B, 64, 1]\n",
    "        )\n",
    "        self.classifier = nn.Linear(64, 1)  # -> [B, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x).squeeze(-1)  # [B, 64]\n",
    "        logit = self.classifier(x)        # [B, 1]\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b03c4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, loss_fn, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)                  # [B, 1, L]\n",
    "        y = y.to(device).float().unsqueeze(1)  # [B, 1]\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)                  # [B, 1]\n",
    "        loss = loss_fn(logit, y)\n",
    "        loss.backward()\n",
    "        if grad_clip is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_probs(model, loader, device):\n",
    "    model.eval()\n",
    "    probs, ys = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        logit = model(x)\n",
    "        p = torch.sigmoid(logit).squeeze(1).cpu().numpy()\n",
    "        probs.append(p)\n",
    "        ys.append(y.numpy())\n",
    "    return np.concatenate(ys), np.concatenate(probs)\n",
    "\n",
    "def evaluate_probs(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:\n",
    "    y_true = y_true.astype(int)\n",
    "    # AUROC can fail if only one class present; guard it\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except Exception:\n",
    "        auc = float(\"nan\")\n",
    "    # default threshold 0.5 for reporting\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, preds, average=\"binary\", zero_division=0)\n",
    "    return {\"auc\": auc, \"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd5c7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = TrainConfig()\n",
    "\n",
    "eeg_dataset = CustomEEGDataset(cfg.csv_path)\n",
    "train_dataset, test_dataset = random_split(eeg_dataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "neg = (train_dataset[:][1] == 0).sum()\n",
    "pos = (train_dataset[:][1] == 1).sum()\n",
    "pos_weight = torch.tensor([neg / max(pos, 1)], dtype=torch.float32, device=cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "98c39f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyEEGCNN().to(cfg.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "53bb651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] loss=0.1731 test_f1=0.951 test_auc=0.998 test_acc=0.979\n",
      "[02] loss=0.1402 test_f1=0.959 test_auc=0.998 test_acc=0.983\n",
      "[03] loss=0.1114 test_f1=0.963 test_auc=0.998 test_acc=0.985\n",
      "[04] loss=0.1150 test_f1=0.959 test_auc=0.999 test_acc=0.983\n",
      "[05] loss=0.1009 test_f1=0.954 test_auc=0.998 test_acc=0.980\n",
      "[06] loss=0.1006 test_f1=0.962 test_auc=0.998 test_acc=0.984\n",
      "[07] loss=0.0829 test_f1=0.969 test_auc=0.999 test_acc=0.987\n",
      "[08] loss=0.0854 test_f1=0.968 test_auc=0.999 test_acc=0.987\n",
      "[09] loss=0.0781 test_f1=0.974 test_auc=0.999 test_acc=0.989\n",
      "[10] loss=0.0860 test_f1=0.977 test_auc=0.999 test_acc=0.990\n",
      "[11] loss=0.0827 test_f1=0.972 test_auc=0.999 test_acc=0.988\n",
      "[12] loss=0.0817 test_f1=0.975 test_auc=0.999 test_acc=0.990\n",
      "[13] loss=0.0733 test_f1=0.963 test_auc=0.999 test_acc=0.984\n",
      "[14] loss=0.0707 test_f1=0.981 test_auc=0.999 test_acc=0.992\n",
      "[15] loss=0.0689 test_f1=0.977 test_auc=0.999 test_acc=0.990\n",
      "[16] loss=0.0654 test_f1=0.978 test_auc=0.999 test_acc=0.991\n",
      "[17] loss=0.0664 test_f1=0.973 test_auc=0.999 test_acc=0.989\n",
      "[18] loss=0.0618 test_f1=0.980 test_auc=0.999 test_acc=0.992\n",
      "[19] loss=0.0685 test_f1=0.976 test_auc=0.999 test_acc=0.990\n",
      "[20] loss=0.0576 test_f1=0.980 test_auc=0.999 test_acc=0.992\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, cfg.device, cfg.grad_clip)\n",
    "    y_test, p_test = infer_probs(model, test_loader, cfg.device)\n",
    "    metrics = evaluate_probs(y_test, p_test)\n",
    "\n",
    "    scheduler.step(metrics[\"f1\"])  # step on F1 (or metrics[\"auc\"])\n",
    "\n",
    "    print(f\"[{epoch:02d}] loss={train_loss:.4f} test_f1={metrics['f1']:.3f} test_auc={metrics['auc']:.3f} \"\n",
    "            f\"test_acc={metrics['acc']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c557f39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
